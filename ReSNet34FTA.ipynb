{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtIyFGAuXEq5uLqMFoYihL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rafi076/RTFER/blob/main/ReSNet34FTA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 0 — Setup & Imports"
      ],
      "metadata": {
        "id": "qtOossuhIYOD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFnD893TIP4d",
        "outputId": "fa6a1890-a29e-4d8a-bb17-36ebaac28ea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Block 0: Setup & Imports\n",
        "import os, zipfile, random, copy\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets, models\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "\n",
        "# Reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 1 — Access ZIP & Detect Folders"
      ],
      "metadata": {
        "id": "pxAe1X8yIbyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Access ZIP & Detect Folders (robust)\n",
        "ZIP_PATH = \"/content/FER-2013.zip\"\n",
        "ROOT     = \"/content/FER-2013\"   # extraction root\n",
        "\n",
        "# 1) Unzip once\n",
        "if os.path.exists(ZIP_PATH) and not os.path.isdir(ROOT):\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "        z.extractall(ROOT)\n",
        "    print(\"Unzipped FER-2013 to:\", ROOT)\n",
        "\n",
        "# 2) Find the DEEPEST folder that actually contains 'train' or 'test'\n",
        "def find_base(start):\n",
        "    candidates = []\n",
        "    for root, dirs, files in os.walk(start):\n",
        "        if \"train\" in dirs or \"test\" in dirs or \"PublicTest\" in dirs or \"PrivateTest\" in dirs:\n",
        "            candidates.append(root)\n",
        "    if not candidates:\n",
        "        return start\n",
        "    # pick the deepest path\n",
        "    return sorted(candidates, key=lambda p: len(p.split(\"/\")))[-1]\n",
        "\n",
        "BASE = find_base(ROOT)\n",
        "print(\"Base folder detected:\", BASE)\n",
        "\n",
        "# 3) Prefer official split names if present\n",
        "TRAIN_DIR_OFF   = os.path.join(BASE, \"train\")\n",
        "PUBLIC_DIR_OFF  = os.path.join(BASE, \"PublicTest\")\n",
        "PRIVATE_DIR_OFF = os.path.join(BASE, \"PrivateTest\")\n",
        "\n",
        "HAS_OFFICIAL = all(os.path.isdir(p) for p in [TRAIN_DIR_OFF, PUBLIC_DIR_OFF, PRIVATE_DIR_OFF])\n",
        "print(\"Official splits found:\", HAS_OFFICIAL)\n",
        "\n",
        "# 4) Fallback (your ZIP has only train/test)\n",
        "TRAIN_SPLIT = os.path.join(BASE, \"train_split\")\n",
        "VAL_SPLIT   = os.path.join(BASE, \"val_split\")\n",
        "TEST_DIR_FALLBACK = os.path.join(BASE, \"test\")\n",
        "\n",
        "if HAS_OFFICIAL:\n",
        "    TRAIN_DIR = TRAIN_DIR_OFF\n",
        "    VAL_DIR   = PUBLIC_DIR_OFF\n",
        "    TEST_DIR  = PRIVATE_DIR_OFF  # FINAL TEST\n",
        "else:\n",
        "    TRAIN_DIR = TRAIN_SPLIT if os.path.isdir(TRAIN_SPLIT) else TRAIN_DIR_OFF\n",
        "    VAL_DIR   = VAL_SPLIT   if os.path.isdir(VAL_SPLIT)   else PUBLIC_DIR_OFF if os.path.isdir(PUBLIC_DIR_OFF) else TRAIN_DIR  # (val==train only if nothing else)\n",
        "    TEST_DIR  = TEST_DIR_FALLBACK\n",
        "\n",
        "print(\"TRAIN_DIR:\", TRAIN_DIR)\n",
        "print(\"VAL_DIR:  \", VAL_DIR)\n",
        "print(\"TEST_DIR: \", TEST_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxKZt3vJIciR",
        "outputId": "dd9cca02-6f95-463d-c8a1-da26da572105"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base folder detected: /content/FER-2013/FER-2013\n",
            "Official splits found: False\n",
            "TRAIN_DIR: /content/FER-2013/FER-2013/train\n",
            "VAL_DIR:   /content/FER-2013/FER-2013/train\n",
            "TEST_DIR:  /content/FER-2013/FER-2013/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 2 — Quick Sanity Check (counts)**"
      ],
      "metadata": {
        "id": "vYTfVSGpIeZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Sanity Check: class names and image counts\n",
        "def count_images(root):\n",
        "    total = 0\n",
        "    per_class = {}\n",
        "    if not os.path.isdir(root):\n",
        "        return 0, {}\n",
        "    for cls in sorted(os.listdir(root)):\n",
        "        p = os.path.join(root, cls)\n",
        "        if os.path.isdir(p):\n",
        "            n = len([f for f in os.listdir(p) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "            per_class[cls] = n\n",
        "            total += n\n",
        "    return total, per_class\n",
        "\n",
        "for name, path in [(\"Train\", TRAIN_DIR), (\"PublicTest/Val\", VAL_DIR), (\"PrivateTest/Final\", TEST_DIR)]:\n",
        "    tot, pc = count_images(path)\n",
        "    print(f\"{name}: total={tot}, per_class={pc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyDyUJbaIi7n",
        "outputId": "717111a4-be62-4159-837b-33e83f42355a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: total=28709, per_class={'angry': 3995, 'disgust': 436, 'fear': 4097, 'happy': 7215, 'neutral': 4965, 'sad': 4830, 'surprise': 3171}\n",
            "PublicTest/Val: total=28709, per_class={'angry': 3995, 'disgust': 436, 'fear': 4097, 'happy': 7215, 'neutral': 4965, 'sad': 4830, 'surprise': 3171}\n",
            "PrivateTest/Final: total=7178, per_class={'angry': 958, 'disgust': 111, 'fear': 1024, 'happy': 1774, 'neutral': 1233, 'sad': 1247, 'surprise': 831}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 3 — Transforms (CLAHE + Augs + TenCrop)**"
      ],
      "metadata": {
        "id": "YrWMK8VzIlTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Transforms\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
        "\n",
        "# CLAHE -> returns a PIL RGB image\n",
        "class CLAHE_PIL(object):\n",
        "    def __init__(self, clip=2.0, grid=(8,8)):\n",
        "        self.clip = clip; self.grid = grid\n",
        "    def __call__(self, img: Image.Image):\n",
        "        g = np.array(img.convert(\"L\"))\n",
        "        clahe = cv2.createCLAHE(clipLimit=self.clip, tileGridSize=self.grid)\n",
        "        g = clahe.apply(g)\n",
        "        rgb = cv2.cvtColor(g, cv2.COLOR_GRAY2RGB)\n",
        "        return Image.fromarray(rgb)\n",
        "\n",
        "# Train pipeline: CLAHE + strong augs\n",
        "train_transform = transforms.Compose([\n",
        "    CLAHE_PIL(clip=2.0, grid=(8,8)),\n",
        "    transforms.Resize(56),\n",
        "    transforms.RandomResizedCrop(48, scale=(0.8, 1.2)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
        "])\n",
        "\n",
        "# Eval pipeline: CLAHE + TenCrop(48); stack 10 crops\n",
        "eval_transform = transforms.Compose([\n",
        "    CLAHE_PIL(clip=2.0, grid=(8,8)),\n",
        "    transforms.Resize(56),\n",
        "    transforms.TenCrop(48),\n",
        "    transforms.Lambda(lambda crops: torch.stack([\n",
        "        transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)(\n",
        "            transforms.ToTensor()(c)\n",
        "        ) for c in crops\n",
        "    ])),\n",
        "])\n"
      ],
      "metadata": {
        "id": "61N92ODuImjf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 4 — Datasets & Dataloaders**"
      ],
      "metadata": {
        "id": "4kQ8RbEBIoLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 4: Datasets & Dataloaders (fixed class mapping)\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "assert os.path.isdir(TRAIN_DIR), f\"Missing TRAIN_DIR: {TRAIN_DIR}\"\n",
        "assert os.path.isdir(TEST_DIR),  f\"Missing TEST_DIR: {TEST_DIR}\"\n",
        "assert os.path.isdir(VAL_DIR),   f\"Missing VAL_DIR: {VAL_DIR}\"\n",
        "\n",
        "# Build datasets\n",
        "train_ds = datasets.ImageFolder(TRAIN_DIR, transform=train_transform)\n",
        "\n",
        "# Build eval datasets first (with their own classes)\n",
        "val_ds   = datasets.ImageFolder(VAL_DIR,   transform=eval_transform)\n",
        "test_ds  = datasets.ImageFolder(TEST_DIR,  transform=eval_transform)\n",
        "\n",
        "# ---- FIX: force a consistent class mapping using the TRAIN classes ----\n",
        "fixed_classes = train_ds.classes\n",
        "fixed_map = {cls: i for i, cls in enumerate(fixed_classes)}\n",
        "\n",
        "def remap_dataset_targets(ds, fixed_map):\n",
        "    # Translate local idx -> global idx using class names\n",
        "    local_classes = ds.classes\n",
        "    translate = {i: fixed_map[c] for i, c in enumerate(local_classes) if c in fixed_map}\n",
        "    # remap targets\n",
        "    if hasattr(ds, \"targets\"):\n",
        "        ds.targets = [translate[t] for t in ds.targets]\n",
        "    # For older torchvision, samples = [(path, target), ...]\n",
        "    if hasattr(ds, \"samples\"):\n",
        "        ds.samples = [(p, translate[t]) for (p, t) in ds.samples]\n",
        "    # override classes and class_to_idx to fixed ones (for reports)\n",
        "    ds.classes = list(fixed_map.keys())\n",
        "    ds.class_to_idx = dict(fixed_map)\n",
        "    return ds\n",
        "\n",
        "val_ds  = remap_dataset_targets(val_ds,  fixed_map)\n",
        "test_ds = remap_dataset_targets(test_ds, fixed_map)\n",
        "\n",
        "print(\"Fixed classes:\", fixed_classes)\n",
        "print(\"Train n:\", len(train_ds), \"| Val n:\", len(val_ds), \"| Test n:\", len(test_ds))\n",
        "\n",
        "# Dataloaders\n",
        "BATCH_TRAIN = 64\n",
        "BATCH_EVAL  = 32\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_EVAL,  shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_EVAL,  shuffle=False, num_workers=2, pin_memory=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqHnBJnkIpRO",
        "outputId": "71cf9286-1005-40a3-aa88-65c76ded2335"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
            "Train n: 28709 | Val n: 28709 | Test n: 7178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 5 — Model (ResNet34) + Loss/Opt/Scheduler**"
      ],
      "metadata": {
        "id": "n2PiPbCTIrii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 5: Model + Loss/Opt/Scheduler\n",
        "class EmotionResNet34(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.BatchNorm1d(in_features),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "        self._init_weights()\n",
        "    def _init_weights(self):\n",
        "        for m in self.classifier:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "    def forward(self, x):\n",
        "        feat = self.backbone(x)\n",
        "        out  = self.classifier(feat)\n",
        "        return out\n",
        "\n",
        "model = EmotionResNet34().to(device)\n",
        "\n",
        "# Label smoothing loss\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1.0 - smoothing\n",
        "    def forward(self, pred, target):\n",
        "        log_probs = F.log_softmax(pred, dim=-1)\n",
        "        n = pred.size(1)\n",
        "        true_dist = torch.zeros_like(log_probs)\n",
        "        true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "        true_dist += self.smoothing / n\n",
        "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))\n",
        "\n",
        "criterion = LabelSmoothingLoss(0.1)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "scaler    = torch.cuda.amp.GradScaler()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37kV6yYlIsd2",
        "outputId": "8f9f712e-1341-49a3-ec42-86b335e88360"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2156613188.py:52: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler    = torch.cuda.amp.GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 6 — Evaluation Utility (TenCrop averaging)**"
      ],
      "metadata": {
        "id": "avU0UygpIuDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 6: Evaluation with TenCrop averaging\n",
        "@torch.no_grad()\n",
        "def evaluate_tencrop(model, loader):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    for images, labels in loader:\n",
        "        # images: [B, 10, C, H, W]\n",
        "        bs, ncrops, c, h, w = images.size()\n",
        "        images = images.view(-1, c, h, w).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(images)              # [B*10, 7]\n",
        "        logits = logits.view(bs, ncrops, -1).mean(1)  # avg over 10 crops\n",
        "        preds = logits.argmax(1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total   += labels.size(0)\n",
        "    return correct / total\n"
      ],
      "metadata": {
        "id": "20Cm5uZSIvAG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper for CPU"
      ],
      "metadata": {
        "id": "7N0c-_ytpgtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fast validation: TenCrop but only first N batches\n",
        "@torch.no_grad()\n",
        "def evaluate_tencrop_subset(model, loader, max_batches=20):\n",
        "    model.eval()\n",
        "    total, correct, seen = 0, 0, 0\n",
        "    for b_idx, (images, labels) in enumerate(loader):\n",
        "        if max_batches is not None and b_idx >= max_batches:\n",
        "            break\n",
        "        bs, ncrops, c, h, w = images.size()\n",
        "        images = images.view(-1, c, h, w).to(device)\n",
        "        labels = labels.to(device)\n",
        "        with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(images)               # [B*10, 7]\n",
        "        logits = logits.view(bs, ncrops, -1).mean(1)\n",
        "        preds = logits.argmax(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total   += labels.size(0)\n",
        "        seen    += 1\n",
        "    return (correct / total) if total > 0 else 0.0\n"
      ],
      "metadata": {
        "id": "OizGQFndpfN4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 7 — Stage-1 Training (freeze backbone)**   FAST CPU version"
      ],
      "metadata": {
        "id": "qjn71a_3Iwpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 7 (FAST CPU): Stage-1 Training (Freeze backbone)\n",
        "for p in model.backbone.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "EPOCHS_S1   = 8         # fewer epochs\n",
        "PATIENCE_S1 = 3         # early stop quicker\n",
        "VAL_EVERY   = 3         # validate every 3 epochs\n",
        "VAL_MAXB    = 20        # only first 20 batches of val each time\n",
        "\n",
        "wait = 0\n",
        "for epoch in range(EPOCHS_S1):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        correct      += (logits.argmax(1) == y).sum().item()\n",
        "        total        += y.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    train_loss = running_loss / total\n",
        "    train_acc  = correct / total\n",
        "\n",
        "    # Validate less often to save time on CPU\n",
        "    if (epoch + 1) % VAL_EVERY == 0 or epoch == EPOCHS_S1 - 1:\n",
        "        val_acc = evaluate_tencrop_subset(model, val_loader, max_batches=VAL_MAXB)\n",
        "        print(f\"[S1][{epoch+1:02d}/{EPOCHS_S1}] loss={train_loss:.4f} | train_acc={train_acc:.4f} | val_acc={val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(best_wts, \"/content/best_fer_model_stage1.pth\")\n",
        "            print(\"  ↳ Saved best Stage-1 weights.\")\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= PATIENCE_S1:\n",
        "                print(\"  ↳ Early stop Stage-1.\")\n",
        "                break\n",
        "    else:\n",
        "        print(f\"[S1][{epoch+1:02d}/{EPOCHS_S1}] loss={train_loss:.4f} | train_acc={train_acc:.4f} | val_acc=skipped\")\n",
        "\n",
        "# Load best from Stage-1\n",
        "model.load_state_dict(best_wts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq5hUjkLIxpY",
        "outputId": "81e75f32-e88c-4d6b-e84b-cd9984a4804a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S1][01/8] loss=2.1539 | train_acc=0.2148 | val_acc=skipped\n",
            "[S1][02/8] loss=1.8617 | train_acc=0.2544 | val_acc=skipped\n",
            "[S1][03/8] loss=1.8229 | train_acc=0.2731 | val_acc=0.1234\n",
            "  ↳ Saved best Stage-1 weights.\n",
            "[S1][04/8] loss=1.8097 | train_acc=0.2738 | val_acc=skipped\n",
            "[S1][05/8] loss=1.8033 | train_acc=0.2804 | val_acc=skipped\n",
            "[S1][06/8] loss=1.7954 | train_acc=0.2875 | val_acc=0.0016\n",
            "[S1][07/8] loss=1.7914 | train_acc=0.2900 | val_acc=skipped\n",
            "[S1][08/8] loss=1.7866 | train_acc=0.2936 | val_acc=0.0016\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 8 — Stage-2 Fine-Tune (unfreeze backbone)**— FAST CPU version"
      ],
      "metadata": {
        "id": "882HvDngIzvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 8 (FAST CPU): Stage-2 Fine-Tune (Unfreeze only layer4)\n",
        "for name, p in model.backbone.named_parameters():\n",
        "    p.requires_grad = (\"layer4\" in name)  # only last residual block\n",
        "\n",
        "# smaller LR for partial FT\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4, weight_decay=1e-4)\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=8, T_mult=2)\n",
        "\n",
        "best_val_acc_ft = best_val_acc\n",
        "best_wts_ft = copy.deepcopy(model.state_dict())\n",
        "\n",
        "EPOCHS_S2   = 15        # fewer epochs\n",
        "PATIENCE_S2 = 5\n",
        "VAL_EVERY   = 3\n",
        "VAL_MAXB    = 20\n",
        "\n",
        "wait = 0\n",
        "for epoch in range(EPOCHS_S2):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        correct      += (logits.argmax(1) == y).sum().item()\n",
        "        total        += y.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    train_loss = running_loss / total\n",
        "    train_acc  = correct / total\n",
        "\n",
        "    if (epoch + 1) % VAL_EVERY == 0 or epoch == EPOCHS_S2 - 1:\n",
        "        val_acc = evaluate_tencrop_subset(model, val_loader, max_batches=VAL_MAXB)\n",
        "        print(f\"[S2][{epoch+1:02d}/{EPOCHS_S2}] loss={train_loss:.4f} | train_acc={train_acc:.4f} | val_acc={val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc_ft:\n",
        "            best_val_acc_ft = val_acc\n",
        "            best_wts_ft = copy.deepcopy(model.state_dict())\n",
        "            torch.save(best_wts_ft, \"/content/best_fer_model_finetuned.pth\")\n",
        "            print(\"  ↳ Saved best Stage-2 weights.\")\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= PATIENCE_S2:\n",
        "                print(\"  ↳ Early stop Stage-2.\")\n",
        "                break\n",
        "    else:\n",
        "        print(f\"[S2][{epoch+1:02d}/{EPOCHS_S2}] loss={train_loss:.4f} | train_acc={train_acc:.4f} | val_acc=skipped\")\n",
        "\n",
        "# Load best fine-tuned model\n",
        "model.load_state_dict(best_wts_ft)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc-x6icNI0mQ",
        "outputId": "2431ce7f-e75a-41c1-afb8-1da164735d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[S2][01/15] loss=1.7090 | train_acc=0.3533 | val_acc=skipped\n",
            "[S2][02/15] loss=1.6266 | train_acc=0.4067 | val_acc=skipped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 9 — FINAL Test on PrivateTest (with TenCrop TTA)**"
      ],
      "metadata": {
        "id": "qu9OjdcDI2tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 9: FINAL Test (PrivateTest if available)\n",
        "final_ckpt = \"/content/best_fer_model_finetuned.pth\"\n",
        "if not os.path.exists(final_ckpt):\n",
        "    final_ckpt = \"/content/best_fer_model_stage1.pth\"\n",
        "model.load_state_dict(torch.load(final_ckpt, map_location=device))\n",
        "print(\"Loaded best checkpoint:\", final_ckpt)\n",
        "\n",
        "final_test_acc = evaluate_tencrop(model, test_loader)\n",
        "print(f\"\\n FINAL TEST ACCURACY (TenCrop TTA) on {'PrivateTest' if 'PrivateTest' in TEST_DIR else 'test/'}: {final_test_acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "li42U5ZKI3mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 10 — Confusion Matrix on Final Test**"
      ],
      "metadata": {
        "id": "DaWJ_Fo0I5VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 10: Confusion Matrix (optional)\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_all(model, loader):\n",
        "    model.eval(); ys, yh = [], []\n",
        "    for images, labels in loader:\n",
        "        bs, ncrops, c, h, w = images.size()\n",
        "        images = images.view(-1, c, h, w).to(device)\n",
        "        labels = labels.to(device)\n",
        "        with torch.autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            logits = model(images)\n",
        "        logits = logits.view(bs, ncrops, -1).mean(1)\n",
        "        preds = logits.argmax(1)\n",
        "        ys.append(labels.cpu().numpy())\n",
        "        yh.append(preds.cpu().numpy())\n",
        "    return np.concatenate(ys), np.concatenate(yh)\n",
        "\n",
        "y_true, y_pred = predict_all(model, test_loader)\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, target_names=train_ds.classes))\n"
      ],
      "metadata": {
        "id": "UoTDuFIFI6h_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}