{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMksfT+kS+dz+oom+WH83uB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rafi076/RTFER/blob/main/Copy_of_ER2013_Ensemble_EffNetB3_ResNet50_Xception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "15241-8pLfkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed6d02b7-535f-45f0-eeec-f6a7fed90de2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'fer2013' dataset.\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"msambare/fer2013\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Dataset root path:\")\n",
        "print(path)\n",
        "\n",
        "print(\"\\nContents of dataset root:\")\n",
        "print(os.listdir(path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAT63HF5NpBA",
        "outputId": "076f9170-bbbf-4aac-be21-a87bdf563420"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset root path:\n",
            "/kaggle/input/fer2013\n",
            "\n",
            "Contents of dataset root:\n",
            "['test', 'train']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Environment & Library Setup**"
      ],
      "metadata": {
        "id": "TdW0u2ekPMfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core deep learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "\n",
        "# Numerical & data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Image processing\n",
        "import cv2\n",
        "from skimage import exposure, filters\n",
        "\n",
        "# Machine learning utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# System & utility\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Environment check\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R2xzOQ-Ojkt",
        "outputId": "1c3e1854-b182-4b0c-bf1b-b42dbeef7a64"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU Available: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------"
      ],
      "metadata": {
        "id": "NJNZ6W7nYfAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Preprocessing Pipeline**"
      ],
      "metadata": {
        "id": "5-wsnfffRV_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for preprocessing\n",
        "from tensorflow.keras.applications import (\n",
        "    resnet50,\n",
        "    xception,\n",
        "    efficientnet\n",
        ")\n"
      ],
      "metadata": {
        "id": "I7hA81t3RZZR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1: *Face-Aware Center Alignment (lightweight & safe)*\n",
        "\n",
        "FER-2013 faces are mostly centered, so we use center cropping + landmark-free alignment\n",
        "(this avoids unstable landmark detectors on low-res faces)."
      ],
      "metadata": {
        "id": "JskmJwhvRwm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def face_aware_center_align(img, crop_ratio=0.9):\n",
        "    \"\"\"\n",
        "    Light face-aware center alignment using center crop.\n",
        "    Safe for FER-2013.\n",
        "    \"\"\"\n",
        "    h, w = img.shape[:2]\n",
        "    crop_h, crop_w = int(h * crop_ratio), int(w * crop_ratio)\n",
        "\n",
        "    start_x = (w - crop_w) // 2\n",
        "    start_y = (h - crop_h) // 2\n",
        "\n",
        "    return img[start_y:start_y + crop_h, start_x:start_x + crop_w]\n"
      ],
      "metadata": {
        "id": "HK4DXr3fR2Qp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2: *CLAHE (Contrast Enhancement)*"
      ],
      "metadata": {
        "id": "4U9eyGxfSBH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_clahe(gray_img):\n",
        "    clahe = cv2.createCLAHE(\n",
        "        clipLimit=2.0,\n",
        "        tileGridSize=(8, 8)\n",
        "    )\n",
        "    return clahe.apply(gray_img)\n"
      ],
      "metadata": {
        "id": "V1gqy0yTSC5p"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.1 ***Gamma Correction*** (This balances the lighting before you enhance contrast)  &    ***NLM Denoising*** (Upgrade from Gaussian. Gaussian blur can \"muddy\" the edges of the lips. Non-Local Means (NLM) denoising is superior because it removes grain while keeping edges razor-sharp)"
      ],
      "metadata": {
        "id": "T1TRGPmCVlos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_gamma_correction(img, gamma=1.1):\n",
        "    \"\"\"Normalizes global illumination before contrast enhancement.\"\"\"\n",
        "    invGamma = 1.0 / gamma\n",
        "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
        "                      for i in np.arange(0, 256)]).astype(\"uint8\")\n",
        "    return cv2.LUT(img, table)\n",
        "\n",
        "def apply_denoising(img):\n",
        "    \"\"\"Removes digital grain while preserving sharp edges of lips/eyes.\"\"\"\n",
        "    # fastNlMeans is superior to Gaussian for keeping micro-expression edges\n",
        "    return cv2.fastNlMeansDenoising(img, None, h=3, templateWindowSize=7, searchWindowSize=21)"
      ],
      "metadata": {
        "id": "qPrPck7wWJdg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3: *Light Gaussian Smoothing*"
      ],
      "metadata": {
        "id": "PXXaHo4mSPuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_smoothing(img):\n",
        "    return cv2.GaussianBlur(img, (3, 3), 0)\n"
      ],
      "metadata": {
        "id": "-Afa5yDPST1I"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4: *Grayscale → RGB Conversion*"
      ],
      "metadata": {
        "id": "_8djAJ23SYB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gray_to_rgb(gray_img):\n",
        "    return cv2.cvtColor(gray_img, cv2.COLOR_GRAY2RGB)\n"
      ],
      "metadata": {
        "id": "-C601yYgSVVu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.5: *Model-Specific Resize*"
      ],
      "metadata": {
        "id": "SJFmVFjaSlUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_for_model(img, model_name):\n",
        "    if model_name == \"resnet50\":\n",
        "        return cv2.resize(img, (224, 224))\n",
        "    elif model_name == \"xception\":\n",
        "        return cv2.resize(img, (299, 299))\n",
        "    elif model_name == \"efficientnetb3\":\n",
        "        return cv2.resize(img, (300, 300))\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model name\")\n"
      ],
      "metadata": {
        "id": "z0oFtNIfSccd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.6: *Model-Specific Normalization*"
      ],
      "metadata": {
        "id": "848CdvYISsGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_model(img, model_name):\n",
        "    img = img.astype(np.float32)\n",
        "\n",
        "    if model_name == \"resnet50\":\n",
        "        return resnet50.preprocess_input(img)\n",
        "    elif model_name == \"xception\":\n",
        "        return xception.preprocess_input(img)\n",
        "    elif model_name == \"efficientnetb3\":\n",
        "        return efficientnet.preprocess_input(img)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model name\")\n"
      ],
      "metadata": {
        "id": "nWgSgTuzSpou"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.7: *FULL Preprocessing Function*"
      ],
      "metadata": {
        "id": "LKzmS1i8S0Eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path, model_name):\n",
        "    # 1. Read image (FER-2013 grayscale)\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # 2. Face-aware alignment (Your crop)\n",
        "    img = face_aware_center_align(img)\n",
        "\n",
        "    # 3. [NEW] Gamma Correction (Balance lighting FIRST)\n",
        "    img = apply_gamma_correction(img, gamma=1.2)\n",
        "\n",
        "    # 4. [NEW] Denoising (Clean grain BEFORE sharpening)\n",
        "    img = apply_denoising(img)\n",
        "\n",
        "    # 5. CLAHE (Contrast Enhancement - now works on clean pixels)\n",
        "    img = apply_clahe(img)\n",
        "\n",
        "    # 6. Gray → RGB (Convert for ImageNet weights)\n",
        "    img = gray_to_rgb(img)\n",
        "\n",
        "    # 7. Resize (Model-specific)\n",
        "    target_size = (224, 224)\n",
        "    if model_name == \"xception\": target_size = (299, 299)\n",
        "    if model_name == \"efficientnetb3\": target_size = (300, 300)\n",
        "\n",
        "    img = cv2.resize(img, target_size, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    # 8. Model-specific normalization\n",
        "    img = preprocess_for_model(img, model_name)\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "dIkxxC00Swoy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------\n"
      ],
      "metadata": {
        "id": "ZN8mLEnPYY8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Dataset Splitting**"
      ],
      "metadata": {
        "id": "siVajLBqYhiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1: *Define dataset paths*"
      ],
      "metadata": {
        "id": "rEPgmC8KYy9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_ROOT = path  # /kaggle/input/fer2013\n",
        "TRAIN_DIR = os.path.join(DATASET_ROOT, \"train\")\n",
        "TEST_DIR  = os.path.join(DATASET_ROOT, \"test\")\n",
        "\n",
        "print(\"Train dir:\", TRAIN_DIR)\n",
        "print(\"Test dir :\", TEST_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeRGPG5uYZsA",
        "outputId": "a0dc864f-5bec-4390-f0ce-9bf89cb54b49"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dir: /kaggle/input/fer2013/train\n",
            "Test dir : /kaggle/input/fer2013/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2: *Read all training image paths & labels*"
      ],
      "metadata": {
        "id": "Fp0bSXR9YZU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_paths_and_labels(root_dir):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for label_name in sorted(os.listdir(root_dir)):\n",
        "        label_path = os.path.join(root_dir, label_name)\n",
        "        if not os.path.isdir(label_path):\n",
        "            continue\n",
        "        for img_name in os.listdir(label_path):\n",
        "            image_paths.append(os.path.join(label_path, img_name))\n",
        "            labels.append(label_name)\n",
        "    return image_paths, labels\n",
        "\n",
        "train_image_paths, train_labels = load_image_paths_and_labels(TRAIN_DIR)\n",
        "test_image_paths, test_labels = load_image_paths_and_labels(TEST_DIR)\n",
        "\n",
        "print(\"Total training images:\", len(train_image_paths))\n",
        "print(\"Total test images    :\", len(test_image_paths))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiHVTOswYV9h",
        "outputId": "edae757f-c8b5-4ab7-a678-0f028f8418d1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training images: 28709\n",
            "Total test images    : 7178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3: *Encode labels (string → integer*"
      ],
      "metadata": {
        "id": "QEB3vGV8ZEhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hard-coding classes ensures indices NEVER change between splits\n",
        "classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(classes)  # Fit on known labels, NOT folder names\n",
        "\n",
        "train_labels_encoded = label_encoder.transform(train_labels)\n",
        "test_labels_encoded  = label_encoder.transform(test_labels)\n",
        "\n",
        "NUM_CLASSES = len(label_encoder.classes_)\n",
        "print(\"Classes:\", label_encoder.classes_)\n",
        "print(\"Number of classes:\", NUM_CLASSES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBgZbZV7Z77R",
        "outputId": "997e26d9-4410-469b-d760-2c0c66fd2056"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'surprise']\n",
            "Number of classes: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4: Split Train → Train (70%) + Validation (15%)\n",
        "\n",
        "We do:\n",
        "\n",
        "85% → temp (train+val)\n",
        "\n",
        "15% → validation\n",
        "\n",
        "Then split temp into 70% train *italicized text*"
      ],
      "metadata": {
        "id": "IwoV4Q8MaBmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Split off 15% for Validation\n",
        "X_temp, X_val, y_temp, y_val = train_test_split(\n",
        "    train_image_paths,\n",
        "    train_labels_encoded,\n",
        "    test_size=0.15,\n",
        "    stratify=train_labels_encoded,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Further split X_temp to get ~70% final training\n",
        "X_train, X_unused, y_train, y_unused = train_test_split(\n",
        "    X_temp,\n",
        "    y_temp,\n",
        "    test_size=0.1765,  # makes final train ≈70%\n",
        "    stratify=y_temp,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train size     :\", len(X_train))\n",
        "print(\"Validation size:\", len(X_val))\n",
        "print(\"Test size      :\", len(test_image_paths))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc4Fps6yaHMh",
        "outputId": "7c535f7c-7fb7-42c9-c81a-e0bd5eee1f3e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size     : 20095\n",
            "Validation size: 4307\n",
            "Test size      : 7178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.5: *Load TEST set*"
      ],
      "metadata": {
        "id": "mo8SdD_4aexM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(\"Class distribution in Final Train:\", dict(zip(label_encoder.classes_, counts)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zjm6X4ifaj8h",
        "outputId": "3c0613a0-7e81-43d8-e8c5-8d3d05414cd3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in Final Train: {np.str_('angry'): np.int64(2797), np.str_('disgust'): np.int64(306), np.str_('fear'): np.int64(2867), np.str_('happy'): np.int64(5051), np.str_('neutral'): np.int64(3475), np.str_('sad'): np.int64(3380), np.str_('surprise'): np.int64(2219)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------"
      ],
      "metadata": {
        "id": "hnYGSzEqapys"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SrImxMXtaljW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}